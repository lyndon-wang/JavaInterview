## 第 1 讲

**问题：Redis 从 4.0 版本开始，能够支持后台异步执行任务，比如异步删除数据，那么你能在 Redis 功能源码中，找到实现后台任务的代码文件吗？**

关于这个问题，@悟空聊架构、@小五、@Kaito 等不少同学都给出了正确答案。我在这些同学回答的基础上，稍微做了些完善，你可以参考下。

Redis 支持三类后台任务，它们本身是在bio.h文件中定义的，如下所示：

```c
#define BIO_CLOSE_FILE    0    //后台线程关闭文件

#define BIO_AOF_FSYNC     1   //后台线程刷盘

#define BIO_LAZY_FREE     2   //后台线程释放内存
```

那么，在 Redis server 启动时，入口 main 函数会调用 InitServerLast 函数，而 InitServerLast 函数会调用 bioInit 函数，来创建这三类后台线程。这里的 bioInit 函数，则是在bio.c文件中实现的。

而对于这三类后台任务的执行来说，它们是在 bioProcessBackgroundJobs 函数（在 bio.c 文件中）中实现的。其中，BIO_CLOSE_FILE 和 BIO_AOF_FSYNC 这两类后台任务的实现代码，分别对应了 close 函数和 redis_fysnc 函数。而 BIO_LAZY_FREE 任务根据参数不同，对应了 lazyfreeFreeObjectFromBioThread、lazyfreeFreeDatabaseFromBioThread 和 lazyfreeFreeSlotsMapFromBioThread 三处实现代码。而这些代码都是在lazyfree.c文件中实现。

此外，还有一些同学给出了异步删除数据的执行流程和涉及函数，比如，@曾轼麟同学以 unlink 为例，列出了删除操作涉及的两个执行流程，我在这里也分享下。

unlink 实际代码的执行流程如下所示：

- 使用异步删除时的流程：unlinkCommand -> delGenericCommand -> dbAsyncDelete -> dictUnlink -> bioCreateBackgroundJob 创建异步删除任务 -> 后台异步删除。
- 不使用异步删除时的流程：unlinkCommand -> delGenericCommand -> dbAsyncDelete -> dictUnlink -> dictFreeUnlinkedEntry 直接释放内存。

此外，@悟空聊架构同学还提到了在 Redis 6.0 中增加的 IO 多线程。不过，Redis 6.0 中的多 IO 线程，主要是为了利用多核并行读取数据、解析命令，以及写回数据，这些线程其实是和主线程一起工作的，所以我通常还是把它们看作前台的工作线程。

## 第 2 讲

**问题：SDS 字符串在 Redis 内部模块实现中也被广泛使用，你能在 Redis server 和客户端的实现中，找到使用 SDS 字符串的地方吗？**

我们可以直接在全局变量 server 对应的结构体 redisServer 中，查找使用 sds 进行定义的成员变量，其代码如下所示：

```c
struct redisServer {

…

sds aof_buf;

sds aof_child_diff;

…}
```

同样，我们可以在客户端对应的结构体 client 中查找 sds 定义的成员变量，如下代码所示：

```c
typedef struct client {

…

sds querybuf;

sds pending_querybuf;

sds replpreamble;

sds peerid;

…} client;
```

此外，你也要注意的是，在 Redis 中，**键值对的 key 也都是 SDS 字符串**。在执行将键值对插入到全局哈希表的函数 dbAdd（在 db.c 文件）中的时候，键值对的 key 会先被创建为 SDS 字符串，然后再保存到全局哈希表中，你可以看看下面的代码。

```c
void dbAdd(redisDb *db, robj *key, robj *val) {

    sds copy = sdsdup(key->ptr);  //根据redisObject结构中的指针获得实际的key，调用sdsdup将其创建为一个SDS字符串

  int retval = dictAdd(db->dict, copy, val); //将键值对插入哈希表

  …

}
```

## 第 3 讲

**问题：Hash 函数会影响 Hash 表的查询效率及哈希冲突情况，那么，你能从 Redis 的源码中，找到 Hash 表使用的是哪一种 Hash 函数吗？**

关于这个问题，@Kaito、@陌、@可怜大灰狼、@曾轼麟等不少同学都找到了 Hash 函数的实现，在这里我也总结下。

其实，我们在查看哈希表的查询函数 dictFind 时，可以看到它会调用 dictHashKey 函数，来计算键值对 key 的哈希值，如下所示：

```c
dictEntry *dictFind(dict *d, const void *key) {

    ...

    // 计算 key 的哈希值

    h = dictHashKey(d, key);

    ...

}
```

那么，我们进一步看 dictHashKey 函数，可以发现它是在 dict.h 文件中定义的，如下所示：

```c
#define dictHashKey(d, key) (d)->type->hashFunction(key)
```

从代码可以看到，dictHashKey 函数会实际执行哈希表类型相关的 hashFunction，来计算 key 的哈希值。所以，这实际上就和哈希表结构体中的 type 有关了。

这里，我们来看看哈希表对应的数据结构 dict 的定义，如下所示：

```c
typedef struct dict {

    dictType *type;

    ...

} dict;
```

dict 结构体中的成员变量 type 类型是 dictType 结构体，而 dictType 里面，包含了哈希函数的函数指针 hashFunction。

```c
typedef struct dictType {

    uint64_t (*hashFunction)(const void *key);

    ...

} dictType;
```

那么，既然 dictType 里面有哈希函数的指针，所以比较直接的方法，就是去看哈希表在初始化时，是否设置了 dictType 中的哈希函数。

在 Redis server 初始化函数 initServer 中，会对数据库中的主要结构进行初始化，这其中就包括了对全局哈希表的初始化，如下所示：

```c
void initServer(void) {

...

for (j = 0; j < server.dbnum; j++) {

        server.db[j].dict = dictCreate(&dbDictType,NULL);  //初始化全局哈希表

        ...}

}
```

从这里，你就可以看到**全局哈希表对应的哈希表类型是 dbDictType**，而 dbDictType 是在server.c文件中定义的，它设置的哈希函数是 dictSdsHash（在 server.c 文件中），如下所示：

```c
dictType dbDictType = {

    dictSdsHash,                //哈希函数

    ...

};
```

我们再进一步查看 dictSdsHash 函数的实现，可以发现它会调用 dictGenHashFunction 函数（在 dict.c 文件中），而 dictGenHashFunction 函数又会进一步调用 siphash 函数（在 siphash.c 文件中）来实际执行哈希计算。所以到这里，我们就可以知道全局哈希表使用的哈希函数是 **siphash**。

下面的代码展示了 dictSdsHash 函数及其调用的关系，你可以看下。

```c
uint64_t dictSdsHash(const void *key) {

    return dictGenHashFunction((unsigned char*)key, sdslen((char*)key));

}

uint64_t dictGenHashFunction(const void *key, int len) {

    return siphash(key,len,dict_hash_function_seed);

}
```

其实，Redis 源码中很多地方都使用到了哈希表，它们的类型有所不同，相应的它们使用的哈希函数也有区别。在 server.c 文件中你可以看到有很多哈希表类型的定义，这里面就包含了不同类型哈希表使用的哈希函数，你可以进一步阅读源码看看。以下代码也展示了一些哈希表类型的定义，你可以看下。

```c
dictType objectKeyPointerValueDictType = {

    dictEncObjHash,

    ...

}

dictType setDictType = {

    dictSdsHash,

    ...

}

dictType commandTableDictType = {

    dictSdsCaseHash, 

    ...

}
```

## 第 4 讲

**问题：SDS 判断是否使用嵌入式字符串的条件是 44 字节，你知道为什么是 44 字节吗？**

这个问题，不少同学都是直接分析了 redisObject 和 SDS 的数据结构，作出了正确的解答。从留言中，也能看到同学们对 Redis 代码的熟悉程度是越来越高了。这里，我来总结下。

嵌入式字符串本身会把 redisObject 和 SDS 作为一个连续区域来分配内存，而就像 @曾轼麟同学在解答时提到的，我们在考虑内存分配问题时，需要了解**内存分配器的工作机制**。那么，对于 Redis 使用的 jemalloc 内存分配器来说，它为了减少内存碎片，并不会按照实际申请多少空间就分配多少空间。

其实，jemalloc 会根据申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数来作为实际的分配空间大小，这样一来，既可以减少内存碎片，也能避免频繁的分配。在使用 jemalloc 时，它的常见分配大小包括 8、16、32、64 等字节。

对于 redisObject 来说，它的结构体是定义在server.h文件中，如下所示：

```c
typedef struct redisObject {

    unsigned type:4;   // 4 bits

    unsigned encoding:   //4 bits

    unsigned lru:LRU_BITS;  //24 bits

    int refcount;  //4字节

    void *ptr;   //8字节

} robj;
```

从代码中可以看到，redisObject 本身占据了 16 个字节的空间大小。而嵌入式字符串对应的 SDS 结构体 sdshdr8，它的成员变量 len、alloc 和 flags 一共占据了 3 个字节。另外它包含的字符数组 buf 中，还会包括一个结束符“\0”，占用 1 个字符。所以，这些加起来一共是 4 个字节。

```c
struct __attribute__ ((__packed__)) sdshdr8 {

    uint8_t len;       // 1字节

    uint8_t alloc;    // 1字节

    unsigned char flags; // 1字节

    char buf[];   //字符数组末尾有一个结束符，占1个字节

};
```

对于嵌入式字符串来说，jemalloc 给它分配的最大大小是 64 个字节，而这其中，redisObject、sdshdr 结构体元数据和字符数组结束符，已经占了 20 个字节，所以这样算下来，嵌入式字符串剩余的空间大小，最大就是 44 字节了（64-20=44）。这也是 SDS 判断是否使用嵌入式字符串的条件是 44 字节的原因。

## 第 5 讲

**问题：在使用跳表和哈希表相结合的双索引机制时，在获得高效范围查询和单点查询的同时，你能想到这种双索引机制有哪些不足之处吗？**

其实，对于双索引机制来说，它的好处很明显，就是可以充分利用不同索引机制的访问特性，来提供高效的数据查找。但是，双索引机制的不足也比较明显，它要占用的空间比单索引要求的更多，这也是因为它需要维护两个索引结构，难以避免会占用较多的内存空间。

我看到有不少同学都提到了“以空间换时间”这一设计选择，我能感觉到大家已经开始注意透过设计方案，去思考和抓住设计的本质思路了，这一点非常棒！**因为很多优秀的系统设计，其实背后就是计算机系统中很朴素的设计思想。**如果你能有意识地积累这些设计思想，并基于这些思想去把握自己的系统设计核心出发点，那么，这可以让你对系统的设计和实现有一个更好的全局观，在你要做设计取舍时，也可以帮助你做决断。

就像这里的“以空间换时间”的设计思想，本身很朴素。而一旦你能抓住这个本质思想后，就可以根据自己系统对内存空间和访问时间哪一个要求更高，来决定是否采用双索引机制。

不过，这里我也想再提醒你**注意一个关键点**：对于双索引结构的更新来说，我们需要保证两个索引结构的一致性，不能出现一个索引结构更新了，而另一个索引没有相应的更新。比如，我们只更新了 Hash，而没有更新跳表。这样一来，就会导致程序能在哈希上找到数据，但是进行范围查询时，就没法在跳表上找到相应的数据了。

对于 Redis 来说，因为它的主线程是单线程，而且它的索引结构本身是不做持久化的，所以双索引结构的一致性保证问题在 Redis 中不明显。但是，一旦在多线程的系统中，有多个线程会并发操作访问双索引时，这个一致性保证就显得很重要了。

如果我们采用同步的方式更新两个索引结构，这通常会对两个索引结构做加锁操作，保证更新的原子性，但是这会阻塞并发访问的线程，造成整体访问性能下降。不过，如果我们采用异步的方式更新两个索引结构，这会减少对并发线程的阻塞，但是可能导致两个索引结构上的数据不一致，而出现访问出错的情况。所以，在多线程情况下对双索引的更新是要重点考虑的设计问题。

另外，在同学们的解答中，我还看到 @陌同学提到了一个观点，他把 skiplist + hash 实现的有序集合和 double linked list + hash 实现的 LRU 管理，进行了类比。其实，对 LRU 来说，它是使用链表结构来管理 LRU 链上的数据，从而实现 LRU 所要求的，数据根据访问时效性进行移动。而与此同时，使用的 Hash 可以帮助程序能在 O(1) 的时间复杂度内获取数据，从而加速数据的访问。

我觉得 @陌同学的这个关联类比非常好，这本身也的确是组合式多数据结构协作，完成系统功能的一个典型体现。

## 第 6 讲

**问题：ziplist 会使用 zipTryEncoding 函数，计算插入元素所需的新增内存空间。那么，假设插入的一个元素是整数，你知道 ziplist 能支持的最大整数是多大吗？**

ziplist 的 zipTryEncoding 函数会判断整数的长度，如果整数长度大于等于 32 位时，zipTryEncoding 函数就不将其作为整数计算长度了，而是作为字符串来计算长度了，所以最大整数是 2 的 32 次方。这部分代码如下所示：

```c
int zipTryEncoding(unsigned char *entry, unsigned int entrylen, long long *v, unsigned char *encoding) {

  …

  //如果插入元素的长度entrylen大于等于32，直接返回0，表示将插入元素作为字符串处理

  if (entrylen >= 32 || entrylen == 0) return 0;

  …

}
```

## 第 7 讲

**问题：作为有序索引，Radix Tree 也能提供范围查询，那么与我们日常使用的 B+ 树，以及**第 5 讲**中介绍的跳表相比，你觉得 Radix Tree 有什么优势和不足吗？**

对于这道题，有不少同学比如 @Kaito、@曾轼麟等同学，都对 Radix Tree、B+ 树和跳表做了对比，这里我就来总结一下。

**Radix Tree 的优势**

- 因为 Radix Tree 是前缀树，所以，当保存数据的 key 具有相同前缀时，Radix Tree 会在不同的 key 间共享这些前缀，这样一来，和 B+ 树、跳表相比，就节省内存空间。
- Radix Tree 在查询单个 key 时，其查询复杂度 O(K) 只和 key 的长度 k 有关，和现存的总数据量无关。而 B+ 树、跳表的查询复杂度和数据规模有关，所以 Radix Tree 查询单个 key 的效率要高于 B+ 树、跳表。
- Radix Tree 适合保存大量具有相同前缀的数据。比如一个典型场景，就是 Linux 内核中的 page cache，使用了 Radix Tree 保存文件内部偏移位置和缓存页的对应关系，其中树上的 key 就是文件中的偏移值。

**Radix Tree 的不足**

- 一般在实现 Radix Tree 时，每个叶子节点就保存一个 key，它的范围查询性能没有 B+ 树和跳表好。这是因为 B+ 树，它的叶子节点可以保存多个 key，而对于跳表来说，它可以遍历有序链表。因此，它们可以更快地支持范围查询。
- Radix Tree 的原理较为复杂，实现复杂度要高于 B+ 树和跳表。

## 第 8 讲

**问题：Redis 源码的 main 函数在调用 initServer 函数之前，会执行如下的代码片段，你知道这个代码片段的作用是什么吗？**

```c
int main(int argc, char **argv) {

...

server.supervised = redisIsSupervised(server.supervised_mode);

int background = server.daemonize && !server.supervised;

if (background) daemonize();

...

}
```

这段代码的目的呢，是先检查 Redis 是否设置成让 upstart 或 systemd 这样的系统管理工具，来启停 Redis。这是由 **redisIsSupervised 函数**，来检查 redis.conf 配置文件中的 supervised 选项，而 supervised 选项的可用配置值，包括 no、upstart、systemd、auto，其中 no 就表示不用系统管理工具来启停 Redis，其他选项会用系统管理工具。

而如果 Redis 没有设置用系统管理工具，同时又设置了使用守护进程方式（对应配置项 daemonize=yes，server.daemonize 值为 1），那么，这段代码就调用 **daemonize 函数**以守护进程的方式，启动 Redis。

## 第 9 讲

**问题：在 Redis 事件驱动框架代码中，分别使用了 Linux 系统上的 select 和 epoll 两种机制，****你知道为什么****Redis 没有使用 poll 这一机制吗？**

这道题呢，主要是希望你对 select 和 poll 这两个 IO 多路复用机制，有进一步的了解。课程的留言区中有不少同学也都回答正确了，我在这里说下我的答案。

select 机制的本质，是**阻塞式监听存放文件描述符的集合**，当监测到有描述符就绪时，select 会结束监测返回就绪的描述符个数。而 select 机制的不足有两个：一是它对单个进程能监听的描述符数量是有限制的，默认是 1024 个；二是即使 select 监测到有文件描述符就绪，程序还是需要线性扫描描述符集合，才能知道具体是哪些文件描述符就绪了。

而 poll 机制相比于 select 机制，本质其实没有太大区别，它只是把 select 机制中文件描述符数量的限制给取消了，允许进程一次监听超过 1024 个描述符。**在线性扫描描述符集合获得就绪的具体描述符这个操作上，poll 并没有优化改进。**所以，poll 相比 select 改进比较有限。而且，就像 @可怜大灰狼、@Kaito 等同学提到的，select 机制的兼容性好，可以在 Linux 和 Windows 上使用。

也正是因为 poll 机制改进有限，而且它对运行平台的支持度不及 select，所以 Redis 的事件驱动框架就没有使用 poll 机制。在 Linux 上，事件驱动框架直接使用了 epoll，而在 Windows 上，框架则使用的是 select。

不过，这里你也要注意的是，Redis 的 ae.c 文件实现了 **aeWait 函数**，这个函数实际会使用 poll 机制来监测文件描述符。而 aeWait 函数会被 rewriteAppendOnlyFile 函数（在 aof.c 文件中）和 migrateGetSocket 函数（在 cluster.c 文件中）调用。@可怜大灰狼同学在回答思考题时，就提到了这一点。

此外，在解答这道题的时候，@Darren、@陌等同学还进一步回答了 epoll 机制的实现细节，我在这里也简单总结下他们的答案，分享给你。

当某进程调用 epoll_create 方法时，Linux 内核会创建一个 eventpoll 结构体，这个结构体中包含了一个红黑树 rbr 和一个双链表 rdlist，如下所示：

```c
struct eventpoll{

    //红黑树的根节点，树中存放着所有添加到epoll中需要监控的描述符

    struct rb_root rbr;

    //双链表中存放着已经就绪的描述符，会通过epoll_wait返回给调用程序

    struct list_head rdlist;

    ...

}
```

epoll_create 创建了 eventpoll 结构体后，程序调用 epoll_ctl 函数，添加要监听的文件描述符时，这些描述符会被保存在红黑树上。

同时，当有描述符上有事件发生时，一个名为 ep_poll_callback 的函数会被调用。这个函数会把就绪的描述符添加到 rdllist 链表中。而 epoll_wait 函数，会检查 rdlist 链表中是否有描述符添加进来。如果 rdlist 链表不为空，那么 epoll_wait 函数，就会把就绪的描述符返回给调用程序了。

## 第 10 讲

**问题：这节课我们学习了 Reactor 模型，除了 Redis，你还了解什么软件系统使用了 Reactor 模型吗？**

对于这道题，不少同学都给出了使用 Reactor 模型的其他软件系统，比如 @Darren、@Kaito、@曾轼麟、@结冰的水滴等同学。那么，使用 Reator 模型的常见软件系统，实际上还包括 Netty、Nginx、Memcached、Kafka 等等。

在解答这道题的时候，我看到 @Darren 同学做了很好的扩展，回答了 Reactor 模型的三种类型。在这里，我也总结下分享给你。

**类型一：单 reactor 单线程**

在这个类型中，Reactor 模型中的 reactor、acceptor 和 handler 的功能都是由一个线程来执行的。reactor 负责监听客户端事件，一旦有连接事件发生，它会分发给 acceptor，由 acceptor 负责建立连接，然后创建一个 handler，负责处理连接建立后的事件。如果是有非连接的读写事件发生，reactor 将事件分发给 handler 进行处理。handler 负责读取客户端请求，进行业务处理，并最终给客户端返回结果。Redis 就是典型的单 reactor 单线程类型。

**类型二：单 reactor 多线程**

在这个类型中，reactor、acceptor 和 handler 的功能由一个线程来执行，与此同时，会有一个线程池，由若干 worker 线程组成。在监听客户端事件、连接事件处理方面，这个类型和单 rector 单线程是相同的，但是不同之处在于，在单 reactor 多线程类型中，handler 只负责读取请求和写回结果，而具体的业务处理由 worker 线程来完成。

**类型三：主 - 从 Reactor 多线程**

在这个类型中，会有一个主 reactor 线程、多个子 reactor 线程和多个 worker 线程组成的一个线程池。其中，主 reactor 负责监听客户端事件，并在同一个线程中让 acceptor 处理连接事件。一旦连接建立后，主 reactor 会把连接分发给子 reactor 线程，由子 reactor 负责这个连接上的后续事件处理。

那么，子 reactor 会监听客户端连接上的后续事件，有读写事件发生时，它会让在同一个线程中的 handler 读取请求和返回结果，而和单 reactor 多线程类似，具体业务处理，它还是会让线程池中的 worker 线程处理。刚才介绍的 Netty 使用的就是这个类型。

我在下面画了三张图，展示了刚才介绍的三个类型的区别，你可以再整体回顾下。

![img](https://typora-imagehost-1308499275.cos.ap-shanghai.myqcloud.com/2022-11/209f9f5a83a6667c600b4cac7c03a189.jpg)

![img](https://typora-imagehost-1308499275.cos.ap-shanghai.myqcloud.com/2022-11/e46d61c855b7658499439d03d1992f41.jpg)

![img](https://typora-imagehost-1308499275.cos.ap-shanghai.myqcloud.com/2022-11/d7e36a34a7538854f74227f0fc1289c3.jpg)

## 第 11 讲

**问题：已知，Redis 事件驱动框架的 aeApiCreate、aeApiAddEvent 等等这些函数，是对操作系统提供的 IO 多路复用函数进行了封装，具体的 IO 多路复用函数分别是在**ae_epoll.c**，**ae_evport.c**，**ae_kqueue.c**，**ae_select.c**四个代码文件中定义的。那么你知道，Redis 在调用 aeApiCreate、aeApiAddEvent 这些函数时，是根据什么条件来决定，具体调用哪个文件中的 IO 多路复用函数的吗？**

其实，这道题的目的，主要是希望你能通过它进一步了解如何进行跨平台的编程开发。在实际业务场景中，我们开发的系统可能需要在不同的平台上运行，比如 Linux 和 Windows。那么，我们在开发时，就需要用同一套代码支持在不同平台上的执行。

就像 Redis 中使用的 IO 多路复用机制一样，不同平台上支持的 IO 多路复用函数是不一样的。但是，使用这些函数的事件驱动整体框架又可以用一套框架来实现。所以，我们就需要在同一套代码中区分底层平台，从而可以正确地使用该平台对应函数。

对应 Redis 事件驱动框架来说，它是用 aeApiCreate、aeApiAddEven 等函数，封装了不同的 IO 多路复用函数，而在 ae.c 文件的开头部分，它使用了 #ifdef、#else、#endif 等**条件编译指令**，来区分封装的函数应该具体使用哪种 IO 多路复用函数。

下面的代码就展示了刚才介绍的条件编译。

```c
#ifdef HAVE_EVPORT

#include "ae_evport.c"

#else

    #ifdef HAVE_EPOLL

    #include "ae_epoll.c"

    #else

        #ifdef HAVE_KQUEUE

        #include "ae_kqueue.c"

        #else

        #include "ae_select.c"

        #endif

    #endif

#endif
```

从这段代码中我们可以看到，如果 HAVE_EPOLL 宏被定义了，那么，代码就会包含 ae_epoll.c 文件，这也就是说，aeApiCreate、aeApiAddEven、aeApiPoll 这些函数就会调用 epoll_create、epoll_ctl、epoll_wait 这些机制。

类似的，如果 HAVE_KQUEUE 宏被定义了，那么，代码就会包含 ae_kqueue.c 文件，框架函数也会实际调用 kqueue 的机制。

那么，接下来的一个问题就是，**HAVE_EPOLL 、HAVE_KQUEUE** **这些宏又是在哪里被定义的呢？**

其实，它们是在 config.h 文件中定义的。

在 config.h 文件中，代码会判断是否定义了__ linux __ 宏，如果有的话，那么，代码就会定义 HAVE_EPOLL 宏。而如果定义了__ FreeBSD__ 、__ OpenBSD __等宏，那么代码就会定义 HAVE_KQUEUE 宏。

下面的代码展示了 config.h 中的这部分逻辑，你可以看下。

```c
#ifdef __linux__

#define HAVE_EPOLL 1

#endif

 

#if (defined(__APPLE__) && defined(MAC_OS_X_VERSION_10_6)) || defined(__FreeBSD__) || defined(__OpenBSD__) || defined (__NetBSD__)

#define HAVE_KQUEUE 1

#endif
```

好了，到这里，我们就知道了，Redis 源码中是根据__linux__、__FreeBSD__、__OpenBSD__这些宏，来决定当前的运行平台是哪个平台，然后再设置相应的 IO 多路复用函数的宏。**而__ linux __ 、__ FreeBSD__ 、__ OpenBSD__ 这些宏，又是如何定义的呢？**

其实，这就和运行平台上的编译器有关了。编译器会根据所运行的平台提供预定义宏。像刚才的__linux__、__FreeBSD__这些都是属于预定义宏，这些预定义宏的名称都是以“__”两条下划线开头和结尾的。你在 Linux 的系统中，比如 CentOS 或者 Ubuntu，运行如下所示的 gcc 命令，你就可以看到 Linux 中运行的 gcc 编译器，已经提供了__linux__这个预定义宏了。

```c
gcc -dM -E -x c /dev/null | grep linux

#define __linux 1

#define __linux__ 1

#define __gnu_linux_x 1

#define linux 1
```

而如果你在 macOS 的系统中运行如下所示的 gcc 命令，你也能看到 macOS 中运行的 gcc 编译器，已经提供了__APPLE__预定义宏。

```c
gcc -dM -E -x c /dev/null | grep APPLE
#define __APPLE__ 1
```

这样一来，当我们在某个系统平台上使用 gcc 编译 Redis 源码时，就可以根据编译器提供的预定义宏，来决定当前究竟该使用哪个 IO 多路复用函数了。而此时使用的 IO 多路复用函数，也是和 Redis 当前运行的平台是匹配的。

## 第 12 讲

**问题：Redis 后台任务使用了 bio_job 结构体来描述，该结构体用了三个指针变量来表示任务参数，如下所示。那么，如果你创建的任务所需要的参数大于 3 个，你有什么应对方法来传参吗？**

```c
struct bio_job {

    time_t time;

    void *arg1, *arg2, *arg3;  //传递给任务的参数

};
```

这道题其实是需要你了解在 C 函数开发时，如果想传递很多参数该如何处理。

其实，这里我们可以充分利用函数参数中的指针，让指针指向一个结构体，比如数组或哈希表。而数组或哈希表这样的结构体中，就可以保存很多参数了。这样一来，我们就可以通过指针指向结构体来传递多个参数了。

不过，你要注意的是，在函数使用参数时，还需要解析指针指向的结构体，这个会产生一些开销。

## 第 13 讲

**问题：**Redis 多 IO 线程机制使用 startThreadedIO 函数和 stopThreadedIO 函数，来设置 IO 线程激活标识 io_threads_active 为 1 和为 0。此处，这两个函数还会对线程互斥锁数组进行解锁和加锁操作，如下所示。那么，你知道为什么这两个函数要执行解锁和加锁操作吗？

```c
void startThreadedIO(void) {

    ...

    for (int j = 1; j < server.io_threads_num; j++)

        pthread_mutex_unlock(&io_threads_mutex[j]);  //给互斥锁数组中每个线程对应的互斥锁做解锁操作

    server.io_threads_active = 1;

}

 

void stopThreadedIO(void) {

    ...

    for (int j = 1; j < server.io_threads_num; j++)

        pthread_mutex_lock(&io_threads_mutex[j]);  //给互斥锁数组中每个线程对应的互斥锁做加锁操作

    server.io_threads_active = 0;

}
```

我设计这道题的目的，主要是希望你可以了解多线程运行时，如何通过互斥锁来控制线程运行状态的变化。这里我们就来看下线程在运行过程中，是如何使用互斥锁的。通过了解这个过程，你就能知道题目中提到的解锁和加锁操作的目的了。

首先，在初始化和启动多 IO 线程的 **initThreadedIO 函数**中，主线程会先获取每个 IO 线程对应的互斥锁。然后，主线程会创建 IO 线程。当每个 IO 线程启动后，就会运行 IOThreadMain 函数，如下所示：

```c
void initThreadedIO(void) {

   …

   for (int i = 0; i < server.io_threads_num; i++) {

  …

  pthread_mutex_init(&io_threads_mutex[i],NULL);

   io_threads_pending[i] = 0;

   pthread_mutex_lock(&io_threads_mutex[i]); //主线程获取每个IO线程的互斥锁

   if (pthread_create(&tid,NULL,IOThreadMain,(void*)(long)i) != 0) {…} //启动IO线程，线程运行IOThreadMain函数

  …} …}
```

而 IOThreadMain 函数会一直执行一个 **while(1)****的循环流程。在这个流程中，线程又会先执行一个 100 万次的循环，而在这个循环中，线程会一直检查有没有待处理的任务，这些任务的数量是用****io_threads_pending 数组**保存的。

在这个 100 万次的循环中，一旦线程检查到有待处理任务，也就是 io_threads_pending 数组中和当前线程对应的元素值不为 0，那么线程就会跳出这个循环，并根据任务类型进行实际的处理。

下面的代码展示了这部分的逻辑，你可以看下。

```c
void *IOThreadMain(void *myid) {

 …

  while(1) {

       //循环100万次，每次检查有没有待处理的任务

        for (int j = 0; j < 1000000; j++) {

            if (io_threads_pending[id] != 0) break;  //如果有任务就跳出循环

        }

        … //从io_threads_lis中取出待处理任务，根据任务类型，调用相应函数进行处理

        }

…}
```

而如果线程执行了 100 万次循环后，仍然没有任务处理。那么，它就会**调用 pthread_mutex_lock 函数**去获取它对应的互斥锁。但是，就像我刚才给你介绍的，在 initThreadedIO 函数中，主线程已经获得了 IO 线程的互斥锁了。所以，在 IOThreadedMain 函数中，线程会因为无法获得互斥锁，而进入等待状态。此时，线程不会消耗 CPU。

与此同时，主线程在进入事件驱动框架的循环前，会**调用 beforeSleep 函数**，在这个函数中，主线程会进一步调用 handleClientsWithPendingWritesUsingThreads 函数，来给 IO 线程分配待写客户端。

那么，在 handleClientsWithPendingWritesUsingThreads 函数中，如果主线程发现 IO 线程没有被激活的话，它就会**调用 startThreadedIO 函数**。

好了，到这里，startThreadedIO 函数就开始执行了。这个函数中会依次调用 pthread_mutex_unlock 函数，给每个线程对应的锁进行解锁操作。这里，你需要注意的是，startThreadedIO 是在主线程中执行的，而每个 IO 线程的互斥锁也是在 IO 线程初始化时，由主线程获取的。

所以，主线程可以调用 pthread_mutex_unlock 函数来释放每个线程的互斥锁。

一旦主线程释放了线程的互斥锁，那么 IO 线程执行的 IOThreadedMain 函数，就能获得对应的互斥锁。紧接着，IOThreadedMain 函数又会释放释放互斥锁，并继续执行 while(1)，如下所示：

```c
void *IOThreadMain(void *myid) {

 …

  while(1) {

     …

     if (io_threads_pending[id] == 0) {

            pthread_mutex_lock(&io_threads_mutex[id]);  //获得互斥锁

            pthread_mutex_unlock(&io_threads_mutex[id]); //释放互斥锁

            continue;

     }

   …} …}
```

那么，这里就是解答第 13 讲课后思考题的关键所在了。

在 IO 线程释放了互斥锁后，主线程可能正好在执行 handleClientsWithPendingWritesUsingThreads 函数，这个函数中除了刚才介绍的，会根据 IO 线程是否激活来启动 IO 线程之外，它也会**调用 stopThreadedIOIfNeeded 函数，来判断是否需要暂停 IO 线程**。

stopThreadedIOIfNeeded 函数一旦发现待处理任务数，不足 IO 线程数的 2 倍，它就会调用 stopThreadedIO 函数来暂停 IO 线程。

**而暂停 IO 线程的办法，就是让主线程获得线程的互斥锁。**所以，stopThreadedIO 函数就会依次调用 pthread_mutex_lock 函数，来获取每个 IO 线程对应的互斥锁。刚才我们介绍的 IOThreadedMain 函数在获得互斥锁后，紧接着就释放互斥锁，其实就是希望主线程执行的 stopThreadedIO 函数，能在 IO 线程释放锁后的这个时机中，获得线程的互斥锁。

这样一来，因为 IO 线程执行 IOThreadedMain 函数时，会一直运行 while(1) 循环，并且一旦判断当前待处理任务为 0 时，它会去获取互斥锁。而此时，如果主线程已经拿到锁了，那么 IO 线程就只能进入等待状态了，这就相当于暂停了 IO 线程。

这里，你还需要注意的一点是，**stopThreadedIO 函数还会把表示当前 IO 线程激活的标记 io_threads_active 设为 0**，这样一来，主线程的 handleClientsWithPendingWritesUsingThreads 函数在执行时，又会根据这个标记来再次调用 startThreadedIO 启用 IO 线程。而就像刚才我们提到的，startThreadedIO 函数会释放主线程拿的锁，让 IO 线程从等待状态进入运行状态。

关于这道题，不少同学都提到了，题目中所说的加解锁操作是为了控制 IO 线程的启停，而且像是 @土豆种南城同学，还特别强调了 IOThreadedMain 函数中执行的 100 万次循环的作用。

因为这个题目涉及的锁操作在好几个函数间轮流执行，所以，我刚才也是把这个过程的整体流程给你做了解释。下面我也画了一张图，展示了主线程通过加解锁控制 IO 线程启停的基本过程，你可以再整体回顾下。

![img](https://typora-imagehost-1308499275.cos.ap-shanghai.myqcloud.com/2022-11/c3d8dc9c44db04d15975cf8bfa4f0401.jpg)

## 第 14 讲

**问题：**如果我们将命令处理过程中的命令执行也交给多 IO 线程执行，你觉得除了对原子性有影响，会有什么好处或其他不足的影响吗？

这道题主要是希望你能对多线程执行模型的优势和不足，有进一步的思考。

其实，使用多 IO 线程执行命令的好处很直接，就是可以充分利用 CPU 的多核资源，让每个核上的 IO 线程并行处理命令，从而提升整体的吞吐率。

但是，这里你要注意的是，如果多个命令执行时要对同一个数据结构进行写操作，那么，此时也就是多个线程要并发写某个数据结构。为了保证操作正确性，我们就需要使用**互斥方法**，比如加锁，来提供并发控制。

这实际上是使用多 IO 线程时的不足，它会带来两个影响：一个是基于加锁等互斥操作的并发控制，会降低系统整体性能；二个是多线程并发控制的开发与调试比较难，会增加开发者的负担。

## 第 15 讲

**问题：**Redis 源码中提供了 getLRUClock 函数来计算全局 LRU 时钟值，同时键值对的 LRU 时钟值是通过 LRU_CLOCK 函数来获取的，以下代码也展示了 LRU_CLOCK 函数的执行逻辑，这个函数包括了两个分支，一个分支是直接从全局变量 server 的 lruclock 中获取全局时钟值，另一个是调用 getLRUClock 函数获取全局时钟值。

那么你知道，为什么键值对的 LRU 时钟值，不直接通过调用 getLRUClock 函数来获取呢？

```c
unsigned int LRU_CLOCK(void) {

    unsigned int lruclock;

    if (1000/server.hz <= LRU_CLOCK_RESOLUTION) {

        atomicGet(server.lruclock,lruclock);

    } else {

        lruclock = getLRUClock();

    }

    return lruclock;

}
```

这道题有不少同学都给出了正确答案，比如 @可怜大灰狼、@Kaito、@曾轼麟等等。这里我来总结下。

其实，调用 getLRUClock 函数获取全局时钟值，它最终会调用 **gettimeofday** 这个系统调用来获取时间。而系统调用会触发用户态和内核态的切换，会带来微秒级别的开销。

而对于 Redis 来说，它的吞吐率是每秒几万 QPS，所以频繁地执行系统调用，这里面带来的微秒级开销有些大。所以，**Redis 只是以固定频率调用 getLRUClock 函数**，使用系统调用获取全局时钟值，然后将该时钟值赋值给全局变量 server.lruclock。当要获取时钟时，直接从全局变量中获取就行，节省了系统调用的开销。

刚才介绍的这种实现方法，在系统的性能优化过程中是有不错的参考价值的，你可以重点掌握下。

## 第 16 讲

**问题：**LFU 算法在初始化键值对的访问次数时，会将访问次数设置为 LFU_INIT_VAL，它的默认值是 5 次。那么，你能结合这节课介绍的代码，说说如果 LFU_INIT_VAL 设置为 1，会发生什么情况吗？

这道题目主要是希望你能理解 LFU 算法实现时，对键值对访问次数的增加和衰减操作。**LFU_INIT_VAL 会在 LFULogIncr 函数中使用**，如下所示：

```c
uint8_t LFULogIncr(uint8_t counter) {

…

double r = (double)rand()/RAND_MAX;

    double baseval = counter - LFU_INIT_VAL;

    if (baseval < 0) baseval = 0;

    double p = 1.0/(baseval*server.lfu_log_factor+1);

  if (r < p) counter++;

  …}
```

从代码中可以看到，如果 LFU_INIT_VAL 比较小，那么 baseval 值会比较大，这就导致 p 值比较小，那么 counter++ 操作的机会概率就会变小，这也就是说，键值对访问次数 counter 不容易增加。

而另一方面，**LFU 算法在执行时，会调用 LFUDecrAndReturn 函数**，对键值对访问次数 counter 进行衰减操作。counter 值越小，就越容易被衰减后淘汰掉。所以，如果 LFU_INIT_VAL 值设置为 1，就容易导致刚刚写入缓存的键值对很快被淘汰掉。

因此，为了避免这个问题，LFU_INIT_VAL 值就要设置的大一些。

## 第 17 讲

**问题：**freeMemoryIfNeeded 函数在使用后台线程删除被淘汰数据的时候，你觉得在这个过程中，主线程仍然可以处理外部请求吗？

这道题像 @Kaito 等不少同学都给出了正确答案，我在这里总结下，也给你分享一下我的思考过程。

Redis 主线程在执行 freeMemoryIfNeeded 函数时，这个函数确定了淘汰某个 key 之后，会先把这个 key 从全局哈希表中删除。然后，这个函数会在 dbAsyncDelete 函数中，**调用 lazyfreeGetFreeEffort 函数，评估释放内存的代价。**

这个代价的计算，主要考虑的是要释放的键值对是集合时，集合中的元素数量。如果要释放的元素过多，主线程就会在后台线程中执行释放内存操作。此时，主线程就可以继续正常处理客户端请求了。而且因为被淘汰的 key 已从全局哈希表中删除，所以客户端也查询不到这个 key 了，不影响客户端正常操作。

## 第 18 讲

**问题：**你能在 serverCron 函数中，查找到 rdbSaveBackground 函数一共会被调用执行几次么？这又分别对应了什么场景呢？

这道题，我们通过在 serverCron 函数中查找 **rdbSaveBackground 函数**，就可以知道它被调用执行了几次。@曾轼麟同学做了比较详细的查找，我整理了下他的答案，分享给你。

首先，在 serverCron 函数中，它会直接调用 rdbSaveBackground 两次。

**第一次直接调用**是在满足 RDB 生成的条件时，也就是修改的键值对数量和距离上次生成 RDB 的时间满足配置阈值时，serverCron 函数会调用 rdbSaveBackground 函数，创建子进程生成 RDB，如下所示：

```c
if (server.dirty >= sp->changes && server.unixtime-server.lastsave > sp->seconds &&

(server.unixtime-server.lastbgsave_try> CONFIG_BGSAVE_RETRY_DELAY

|| server.lastbgsave_status == C_OK)) {

…

rdbSaveBackground(server.rdb_filename,rsiptr);

…}
```

**第二次直接调用**是在客户端执行了 BGSAVE 命令后，Redis 设置了 rdb_bgsave_scheduled 等于 1，此时，serverCron 函数会检查这个变量值以及当前 RDB 子进程是否运行。

如果子进程没有运行的话，那么 serverCron 函数就调用 rdbSaveBackground 函数，生成 RDB，如下所示：

```c
if (!hasActiveChildProcess() && server.rdb_bgsave_scheduled &&

(server.unixtime-server.lastbgsave_try > CONFIG_BGSAVE_RETRY_DELAY ||

         server.lastbgsave_status == C_OK)) {

 …

if (rdbSaveBackground(server.rdb_filename,rsiptr) == C_OK)

…}
```

而除了刚才介绍的两次直接调用以外，在 serverCron 函数中，还会有两次对 rdbSaveBackground 的**间接调用**。

一次间接调用是通过 replicationCron -> startBgsaveForReplication -> rdbSaveBackground 这个调用关系，来间接调用 rdbSaveBackground 函数，为主从复制定时任务生成 RDB 文件。

另一次间接调用是通过 checkChildrenDone –> backgroundSaveDoneHandler -> backgroundSaveDoneHandlerDisk -> updateSlavesWaitingBgsave -> startBgsaveForReplication -> rdbSaveBackground 这个调用关系，来生成 RDB 文件的。而这个调用主要是考虑到在主从复制过程中，有些从节点在等待当前的 RDB 生成过程结束，因此在当前的 RDB 子进程结束后，这个调用为这些等待的从节点新调度启动一次 RDB 子进程。

## 第 19 讲

**问题：**RDB 文件的创建是由一个子进程来完成的，而 AOF 重写也是由一个子进程完成的，这两个子进程可以各自单独运行。那么请你思考一下，为什么 Redis 源码中在有 RDB 子进程运行时，不会启动 AOF 重写子进程呢？

我设计这道题的目的，是希望你能了解和掌握 RDB 文件创建和 AOF 重写这两个操作本身，涉及到的资源消耗。我们在开发系统软件时，对于使用子进程或是线程来进行并发处理，有时会存在**一个误区：只要使用了多子进程或是多线程就可以加速并行执行的任务。**

但是，执行多子进程能够获得的收益还是要看这些子进程，对资源竞争的情况。就像这道题目提出的，虽然 RDB 创建和 AOF 重写可以会用两个子进程单独运行，但是从它们使用的资源角度来看，它们之间会存在竞争。

那么，一个最明显的资源竞争就是**对磁盘的写竞争**。创建 RDB 文件和重写 AOF，都需要把数据写入磁盘，如果同时让这两个子进程写盘，就会给磁盘带来较大的压力。而除了磁盘资源竞争以外，RDB 文件创建和 AOF 重写还需要读取 Redis 数据库中的所有键值对，如果这两个子进程同时执行，也会消耗 CPU 资源。

## 第 20 讲

**问题：**这节课，我给你介绍了重写子进程和主进程间进行操作命令传输、ACK 信息传递用的三个管道。那么，你在 Redis 源码中还能找到其他使用管道的地方吗？

这道题目，是希望你能更多地了解下管道在 Redis 中的应用。有不少同学都找到了多个使用管道的地方，我在这里总结下。

**首先，创建 RDB、AOF 重写和主从复制时会用到管道。**

在 RDB 文件的创建函数 rdbSaveBackground、AOF 重写的函数 rewriteAppendOnlyFileBackground，以及把 RDB 通过 socket 传给从节点的函数 rdbSaveToSlavesSockets 中，它们都会调用 openChildInfoPipe 函数，创建一个管道 **child_info_pipe**，这个管道的描述符数组，保存在了全局变量 server 中。

当 RDB 创建结束或是 AOF 文件重写结束后，这两个函数会调用 sendChildInfo 函数，通过刚才创建的管道 child_info_pipe，把子进程写时复制的实际数据量发送给父进程。

下面的代码展示了 rdbSaveBackground、rewriteAppendOnlyFileBackground、rdbSaveToSlavesSockets 这三个函数使用管道的主要代码，你可以看下。

```c
int rdbSaveBackground(char *filename, rdbSaveInfo *rsi) {

…

openChildInfoPipe();

if ((childpid = fork()) == 0) {

…

server.child_info_data.cow_size = private_dirty; //记录实际的写时复制数据量

  sendChildInfo(CHILD_INFO_TYPE_RDB); //将写时复制数据量发送给父进程

  …} …}

   

int rdbSaveToSlavesSockets(rdbSaveInfo *rsi) {

…

openChildInfoPipe();

if ((childpid = fork()) == 0) {

…

server.child_info_data.cow_size = private_dirty; //记录实际的写时复制数据量

sendChildInfo(CHILD_INFO_TYPE_RDB); //将写时复制数据量发送给父进程

…} …}

 

int rewriteAppendOnlyFileBackground(void) {

…

openChildInfoPipe();  //创建管道

…

if ((childpid = fork()) == 0) {

…

if (rewriteAppendOnlyFile(tmpfile) == C_OK) {

…

server.child_info_data.cow_size = private_dirty; //记录实际写时复制的数据量

sendChildInfo(CHILD_INFO_TYPE_AOF); //将写时复制的数据量发送给父进程

…} …}

…}
```

此外，在刚才介绍的 rdbSaveToSlavesSockets 函数中，它还会创建一个管道。当子进程把数据传给从节点后，子进程会使用这个管道，向父进程发送成功接收到所有数据传输的从节点 ID，你可以看看下面的代码。

```c
int rdbSaveToSlavesSockets(rdbSaveInfo *rsi) {

…

if (pipe(pipefds) == -1) return C_ERR;

server.rdb_pipe_read_result_from_child = pipefds[0];  //创建管道读端

server.rdb_pipe_write_result_to_parent = pipefds[1]; //创建管道写端

…

if ((childpid = fork()) == 0) {

…

//数据传输完成后，通过管道向父进程传输从节点ID

if (*len == 0 || write(server.rdb_pipe_write_result_to_parent,msg,msglen) != msglen) {…}

…} …}
```

**其次，Redis module 运行时会用到管道。**

在 module 的初始化函数 moduleInitModulesSystem 中，它会创建一个管道 **module_blocked_pipe**，这个管道会用来唤醒由于处理 module 命令而阻塞的客户端。

下面的代码展示了管道在 Redis module 中的使用，你可以看下。

```c
void moduleInitModulesSystem(void) {

...

if (pipe(server.module_blocked_pipe) == -1) {...} //创建管道

...}

int RM_UnblockClient(RedisModuleBlockedClient *bc, void *privdata) {

...

if (write(server.module_blocked_pipe[1],"A",1) != 1) {...} //向管道中写入“A”字符，表示唤醒被module阻塞的客户端

...}

void moduleHandleBlockedClients(void) {

...

while (read(server.module_blocked_pipe[0],buf,1) == 1); //从管道中读取字符

...}
```

**最后，linuxMadvFreeForkBugCheck 函数会用到管道。**

基于 arm64 架构的 Linux 内核有一个 Bug，这个 Bug 可能会导致数据损坏。而 Redis 源码就针对这个 Bug，打了一个补丁，这个补丁在 main 函数的执行过程中，会调用 linuxMadvFreeForkBugCheck 函数，这个函数会 fork 一个子进程来判断是否发现 Bug，而子进程会使用管道来和父进程交互检查结果。你也可以具体看下修复这个 Bug 的补丁。

## 第 21 讲

**问题：**这节课我们介绍的状态机是当实例为从库时会使用的。那么，当一个实例是主库时，为什么不需要使用一个状态机，来实现主库在主从复制时的流程流转呢？

在 Redis 实现主从复制时，从库涉及到的状态变迁有很多，包括了发起连接、主从握手、复制类型判断、请求数据等。因此，使用状态机开发从库的复制流程，可以很好地帮助我们实现状态流转。

但是，如果你再去看下主从复制的启动，你会发现，**主从复制都是由从库执行 slaveof 或 replicaof 命令而开始**。这也就是说，主从复制的发起方是从库，而对于主库来说，它只是**被动式地响应**从库的各种请求，并根据从库的请求执行相应的操作，比如生成 RDB 文件或是传输数据等。

而且，从另外一个角度来说，主库可能和多个从库进行主从复制，而**不同从库的复制进度和状态很可能并不一样**，如果主库要维护状态机的话，那么，它还需要为每个从库维护一个状态机，这个既会增加开发复杂度，也会增加运行时的开销。正是因为这些原因，所以主库并不需要使用状态机进行状态流转。

除此之外， @曾轼麟同学也提到了一个原因，主库本身是可能发生故障，并要进行故障切换的。如果主库在执行主从复制时，也维护状态机，那么**一旦主库发生了故障，也还需要考虑状态机的冗余备份和故障切换**，这会给故障切换的开发和执行带来复杂度和开销。而从库维护状态机本身就已经能完成主从复制，所以没有必要让主库再维护状态机了。

## 第 22 讲

**问题：**哨兵实例本身是有配置文件 sentinel.conf 的，那么你能在哨兵实例的初始化过程中，找到解析这个配置文件的函数吗？

在前面的第 8 讲中，我重点给你介绍了 Redis server 的启动和初始化过程。因为哨兵实例本身也是一个 Redis server，所以它启动后的初始化代码执行路径，和 Redis server 是类似的。

哨兵实例启动后，它的入口函数是 serve.c 文件中的 **main 函数**。然后，main 函数会调用 loadServerConfig 函数加载配置文件。而 loadServerConfig 会进一步调用 loadServerConfigFromString 函数，解析配置文件中的具体配置项。

那么，当 loadServerConfigFromString 函数在解析配置项时，它会使用条件分支判断来匹配不同的配置项。当它匹配到配置项为“**sentinel**”时，它就会执行解析哨兵实例配置项的代码分支了，具体来说，它会**调用 sentinelHandleConfiguration 函数来进行解析**，如下所示：

```c
void loadServerConfigFromString(char *config) {

else if (!strcasecmp(argv[0],"sentinel")) {

…

err = sentinelHandleConfiguration(argv+1,argc-1);

…}…}
```

sentinelHandleConfiguration 函数是在 sentinel.c 文件中实现的，它和 loadServerConfigFromString 函数类似，也是匹配 sentinel.conf 中的不同配置项，进而执行不同的代码分支。你可以进一步阅读它的代码来进行了解。

我在这里也画了一张图，展示了哨兵实例解析配置项的函数调用关系，你可以看下。

![img](https://typora-imagehost-1308499275.cos.ap-shanghai.myqcloud.com/2022-11/9c1d41a0164692e569f95d4db474e8cc.jpg)

## 第 23 讲

**问题：**哨兵实例执行的周期性函数 sentinelTimer，它在函数执行逻辑的最后，会修改 server.hz 配置项，如下所示：

```c
void sentinelTimer(void) {

...

server.hz = CONFIG_DEFAULT_HZ + rand() % CONFIG_DEFAULT_HZ;

}
```

那么，你知道调整 server.hz 的目的是什么吗？

这道题目，像是 @Kaito、@曾轼麟、@可怜大灰狼等不少同学，都给出了正确答案，这里我就来总结一下。

那么，要回答这道题目，首先你要知道 **server.hz 表示的是定时任务函数 serverCron 的执行频率**，而哨兵实例执行的周期性函数 sentinelTimer，也是在 serverCron 中被调用执行的。所以，sentinelTimer 函数的运行频率会按照 server.hz 来执行。

我在第 23 讲中给你介绍过，当哨兵实例判断了主节点客观下线后，它们就要开始选举 Leader 节点，以便进行故障切换。但是，Leader 选举时，哨兵需要获得半数以上的赞成票，如果在一轮选举中没能选出 Leader，此时，哨兵实例会再次进行选举。

但是，为了避免多个哨兵同时开始进行选举，又同时都没法获得超过半数的赞成票，而导致 Leader 选举失败，sentinelTimer 函数在执行的最后一步，对 server.hz 做了微调：**在默认值 CONFIG_DEFAULT_HZ 的基础上，增加一个随机值。**

这样一来，每个哨兵的执行频率就不会完全同步了。一轮选举失败后，哨兵再次选举时，不同哨兵的再次执行频率不一样，这就把它们发起投票的时机错开了，从而降低了它们都无法获得超过半数赞成票的概率，也就保证了 Leader 选举能快速完成，可以执行实际的故障切换。

所以，sentinelTimer 函数修改 server.hz，可以避免故障切换过程中，因为 Leader 节点选举不出来而导致无法完成的情况，提升了 Redis 的可用性。

## 第 24 讲

**问题：**哨兵在 sentinelTimer 函数中会调用 sentinelHandleDictOfRedisInstances 函数，对每个主节点都执行 sentinelHandleRedisInstance 函数，并且还会对主节点的所有从节点，也执行 sentinelHandleRedisInstance 函数。那么，哨兵会判断从节点的主观下线和客观下线吗？

这道题目是希望你能进一步阅读 sentinelHandleRedisInstance 函数的源码，对它的执行流程有个更加详细的了解。

@曾轼麟同学在留言区就给出了比较详细的分析，我在此基础上做了些完善，分享给你。

首先，在 sentinelHandleDictOfRedisInstances 函数中，它会执行一个循环流程，针对当前哨兵实例监听的每个主节点，都**执行 sentinelHandleRedisInstance 函数**。

在这个处理过程中，存在一个**递归调用**，也就是说，如果当前处理的节点就是主节点，那么 sentinelHandleDictOfRedisInstances 函数，会进一步针对这个主节点的从节点，再次调用 sentinelHandleDictOfRedisInstances 函数，从而对每个从节点执行 sentinelHandleRedisInstance 函数。

这部分的代码逻辑如下所示：

```c
void sentinelHandleDictOfRedisInstances(dict *instances) {

…

di = dictGetIterator(instances);

while((de = dictNext(di)) != NULL) {

   sentinelRedisInstance *ri = dictGetVal(de);  //获取哨兵实例监听的每个主节点

   sentinelHandleRedisInstance(ri);  //调用sentinelHandleRedisInstance

   if (ri->flags & SRI_MASTER) {  //如果当前节点是主节点，那么调用sentinelHandleDictOfRedisInstances对它的所有从节点进行处理。

  sentinelHandleDictOfRedisInstances(ri->slaves);

  …}

…}…}
```

然后，在 sentinelHandleRedisInstance 函数执行时，它会**调用 sentinelCheckSubjectivelyDown 函数**，来判断当前处理的实例是否主观下线。这步操作没有任何额外的条件约束，也就是说，无论当前是主节点还是从节点，都会被判断是否主观下线的。这部分代码如下所示：

```c
void sentinelHandleRedisInstance(sentinelRedisInstance *ri) {

…

sentinelCheckSubjectivelyDown(ri);  //无论是主节点还是从节点，都会检查是否主观下线

…}
```

但是要注意，sentinelHandleRedisInstance 函数在调用 sentinelCheckObjectivelyDown 函数，判断实例客观下线状态时，它会检查当前实例是否有主节点标记，如下所示：

```c
void sentinelHandleRedisInstance(sentinelRedisInstance *ri) {

…

  if (ri->flags & SRI_MASTER) {  //只有当前是主节点，才检查是否客观下线

        sentinelCheckObjectivelyDown(ri);

   …}

…}
```

那么总结来说，对于主节点和从节点，它们的 sentinelHandleRedisInstance 函数调用路径就如下所示：

> 主节点：sentinelHandleRedisInstance -> sentinelCheckSubjectivelyDown -> sentinelCheckObjectivelyDown
>
> 从节点：sentinelHandleRedisInstance -> sentinelCheckSubjectivelyDown

所以，回到这道题目的答案上来说，哨兵会判断从节点的主观下线，但不会判断其是否客观下线。

此外，@曾轼麟同学还通过分析代码，看到了**从节点被判断为主观下线后，是不能被选举为新主节点的**。这个过程是在 sentinelSelectSlave 函数中执行的，这个函数会遍历当前的从节点，依次检查它们的标记，如果一个从节点有主观下线标记，那么这个从节点就会被直接跳过，不会被选为新主节点。

下面的代码展示了 sentinelSelectSlave 函数这部分的逻辑，你可以看下。

```c
sentinelRedisInstance *sentinelSelectSlave(sentinelRedisInstance *master) {

…

di = dictGetIterator(master->slaves);

    while((de = dictNext(di)) != NULL) { //遍历主节点的每一个从节点

        sentinelRedisInstance *slave = dictGetVal(de);

        …

        if (slave->flags & (SRI_S_DOWN|SRI_O_DOWN)) continue; //如果从节点主观下线，那么直接跳过该节点，不能被选为新主节点

    …} …}
```



## 第 25 讲

**问题：**如果我们在哨兵实例上执行 publish 命令，那么，这条命令是不是就是由 pubsub.c 文件中的 publishCommand 函数来处理的呢?

这道题目主要是希望你能了解，哨兵实例会使用到哨兵自身实现的命令，而不是普通 Redis 实例使用的命令。这一点我们从哨兵初始化的过程中就可以看到。

哨兵初始化时，会调用 **initSentinel 函数**。而 initSentinel 函数会先把 server.commands 对应的命令表清空，然后执行一个循环，把哨兵自身的命令添加到命令表中。哨兵自身的命令是使用 **sentinelcmds 数组**保存的。

那么从 sentinelcmds 数组中，我们可以看到 publish 命令对应的实现函数，其实是 **sentinelPublishCommand**。所以，我们在哨兵实例上执行 publish 命令，执行的并不是 pubsub.c 文件中的 publishCommand 函数。

下面的代码展示了 initSentinel 函数先清空、再填充命令表的基本过程，以及 sentinelcmds 数组的部分内容，你可以看下。

```c
void initSentinel(void) {

    ...

    dictEmpty(server.commands,NULL);  //清空现有的命令表

    // 将sentinelcmds数组中的命令添加到命令表中

    for (j = 0; j < sizeof(sentinelcmds)/sizeof(sentinelcmds[0]); j++) {

        int retval;

        struct redisCommand *cmd = sentinelcmds+j;

        retval = dictAdd(server.commands, sdsnew(cmd->name), cmd);

        …

    }

    ...}

 

//sentinelcmds数组的部分命令定义

struct redisCommand sentinelcmds[] = {

    ...

    {"subscribe",subscribeCommand,-2,"",0,NULL,0,0,0,0,0},

    {"publish",sentinelPublishCommand,3,"",0,NULL,0,0,0,0,0}, //publish命令对应哨兵自身实现的sentinelPublishCommand函数

    {"info",sentinelInfoCommand,-1,"",0,NULL,0,0,0,0,0},

    ...

};
```

## 第 26 讲

**问题：**在今天课程介绍的源码中，你知道为什么 clusterSendPing 函数计算 wanted 值时，是用的集群节点个数的十分之一吗？

Redis Cluster 在使用 clusterSendPing 函数，检测其他节点的运行状态时，**既需要及时获得节点状态，又不能给集群的正常运行带来过大的额外通信负担。**

因此，clusterSendPing 函数发送的 Ping 消息，其中包含的节点个数不能过多，否则会导致 Ping 消息体较大，给集群通信带来额外的负担，影响正常的请求通信。而如果 Ping 消息包含的节点个数过少，又会导致节点无法及时获知较多其他节点的状态。

所以，wanted 默认设置为集群节点个数的十分之一，主要是为了避免上述两种情况的发生。

## 第 27 讲

**问题：**processCommand 函数在调用完 getNodeByQuery 函数后，实际调用 clusterRedirectClient 函数进行请求重定向前，会根据当前命令是否是 EXEC，分别调用 discardTransaction 和 flagTransaction 两个函数。

那么，你能通过阅读源码，知道这里调用 discardTransaction 和 flagTransaction 的目的是什么吗?

```c
int processCommand(client *c) {

…

clusterNode *n = getNodeByQuery(c,c->cmd,c->argv,c->argc,

                                        &hashslot,&error_code);

if (n == NULL || n != server.cluster->myself) {

   if (c->cmd->proc == execCommand) {

      discardTransaction(c);

   } else {

      flagTransaction (c);

   }

   clusterRedirectClient(c,n,hashslot,error_code);

   return C_OK;

  }

  …

  }
```

这道题目，像 @Kaito、@曾轼麟等同学都给了较为详细的解释，我完善了下他们的答案，分享给你。

首先你要知道，当 Redis Cluster 运行时，它并不支持跨节点的事务执行。那么，我们从题目中的代码中可以看到，当 getNodeByQuery 函数返回 null 结果，或者查询的 key 不在当前实例时，discardTransaction 或 flagTransaction 函数会被调用。

这里你要**注意**，getNodeByQuery 函数返回 null 结果，通常是表示集群不可用、key 找不到对应的 slot、操作的 key 不在同一个 slot 中、key 正在迁移等这些情况。

那么，当这些情况发生，或者是查询的 key 不在当前实例时，如果 client 执行的是 EXEC 命令，**discardTransaction 函数**就会被调用，它会放弃事务的执行，清空当前 client 之前缓存的命令，并对事务中的 key 执行 unWatch 操作，最后重置 client 的事务标记。

而如果当前 client 执行的是事务中的普通命令，那么 **flagTransaction 函数**会被调用。它会给当前 client 设置标记 CLIENT_DIRTY_EXEC。这样一来，当 client 后续执行 EXEC 命令时，就会根据这个标记，放弃事务执行。

总结来说，就是当集群不可用、key 找不到对应的 slot、key 不在当前实例中、操作的 key 不在同一个 slot 中，或者 key 正在迁移等这几种情况发生时，事务的执行都会被放弃。

## 第 28 讲

**问题：**在维护 Redis Cluster 集群状态的数据结构 clusterState 中，有一个字典树 slots_to_keys。当在数据库中插入 key 时它会被更新，你能在 Redis 源码文件 db.c 中，找到更新 slots_to_keys 字典树的相关函数调用吗？

这道题目也有不少同学给出了正确答案，我来给你总结下。

首先，**dbAdd 函数是用来将键值对插入数据库中的**。如果 Redis Cluster 被启用了，那么 dbAdd 函数会调用 slotToKeyAdd 函数，而 slotToKeyAdd 函数会调用 slotToKeyUpdateKey 函数。

那么在 slotToKeyUpdateKey 函数中，它会调用 raxInsert 函数更新 slots_to_keys，调用链如下所示：

> dbAdd -> slotToKeyAdd -> slotToKeyUpdateKey -> raxInsert

然后，**dbAsyncDelete 和 dbSyncDelete 是用来删除键值对的**。如果 Redis Cluster 被启用了，这两个函数都会调用 slotToKeyUpdateKey 函数。而在 slotToKeyUpdateKey 函数里，它会调用 raxRemove 函数更新 slots_to_keys，调用链如下所示：

> dbAsyncDelete/dbSyncDelete -> slotToKeyDel -> slotToKeyUpdateKey -> raxRemove

另外，**empytDb 函数是用来清空数据库的**。它会调用 slotToKeyFlush 函数，并由 slotToKeyFlush 函数，调用 raxFree 函数更新 slots_to_keys，调用链如下所示：

> empytDb -> slotToKeyFlush -> raxFree

还有在 **getKeysInSlot 函数**中，它会调用 raxStart 获得 slots_to_keys 的迭代器，进而查询指定 slot 中的 keys。而在 **delKeysInSlot 函数**中，它也会调用 raxStart 获得 slots_to_keys 的迭代器，并删除指定 slot 中的 keys。

此外，@曾轼麟同学还通过查阅 Redis 源码的 git 历史提交记录，发现 slots_to_keys 原先是使用跳表实现的，后来才替换成字典树。而这一替换的目的，也主要是为了方便通过 slot 快速查找到 slot 中的 keys。

## 第 29 讲

**问题：**在 addReplyReplicationBacklog 函数中，它会计算从节点在全局范围内要跳过的数据长度，如下所示：

```c
skip = offset - server.repl_backlog_off;
```

然后，它会根据这个跳过长度计算实际要读取的数据长度，如下所示：

```c
len = server.repl_backlog_histlen - skip;
```

请你阅读 addReplyReplicationBacklog 函数和调用它的 masterTryPartialResynchronization 函数，你觉得这里的 skip 会大于 repl_backlog_histlen 吗？

其实，在 masterTryPartialResynchronization 函数中，从节点要读取的全局位置对应了变量 psync_offset，这个函数会比较 psync_offset 是否小于 repl_backlog_off，以及 psync_offset 是否大于 repl_backlog_off 加上 repl_backlog_histlen 的和。

当这两种情况发生时，masterTryPartialResynchronization 函数会进行**全量复制**，如下所示：

```c
int masterTryPartialResynchronization(client *c) {

…

// psync_offset小于repl_backlog_off时，或者psync_offset 大于repl_backlog_off加repl_backlog_histlen的和时

if (!server.repl_backlog ||

        psync_offset < server.repl_backlog_off ||

        psync_offset > (server.repl_backlog_off + server.repl_backlog_histlen)) { 

   …

   goto need_full_resync;  //进行全量复制

}
```

当 psync_offset 大于 repl_backlog_off，并且小于 repl_backlog_off 加上 repl_backlog_histlen 的和，此时，masterTryPartialResynchronization 函数会调用 addReplyReplicationBacklog 函数，进行**增量复制**。

而 psync_offset 会作为参数 offset，传给 addReplyReplicationBacklog 函数。因此，在 addReplyReplicationBacklog 函数中计算 skip 时，就不会发生 skip 会大于 repl_backlog_histlen 的情况了，这种情况已经在 masterTryPartialResynchronization 函数中处理了。

## 第 30 讲

**问题：**Redis 在命令执行的 call 函数中，为什么不会针对 EXEC 命令，调用 slowlogPushEntryIfNeeded 函数来记录慢命令呢？

我设计这道题的主要目的，是希望你能理解 EXEC 命令的使用场景和事务执行的过程。

**EXEC 命令是用来执行属于同一个事务的所有命令的**。当程序要执行事务时，会先执行 MULTI 命令，紧接着，执行的命令并不会立即执行，而是被放到一个队列中缓存起来。等到 EXEC 命令执行时，在它之前被缓存起来等待执行的事务命令，才会实际执行。

因此，EXEC 命令执行时，实际上会执行多条事务命令。此时，如果调用 slowlogPushEntryIfNeeded 函数记录了慢命令的话，并不能表示 EXEC 本身就是一个慢命令。而实际可能会耗时长的命令是事务中的命令，并不是 EXEC 命令自身，所以，这里不会针对 EXEC 命令，来调用 slowlogPushEntryIfNeeded 函数。

## 第 31 讲

**问题：**你使用过哪些 Redis 的扩展模块，或者自行开发过扩展模块吗？欢迎分享一些你的经验。

我自己有使用过 Redis 的 **TimeSeries 扩展模块**，用来在一个物联网应用的场景中保存一些时间序列数据。TimeSeries 模块的功能特点是可以使用标签来对不同的数据集合进行过滤，通过集合标签筛选应用需要的集合数据。而且这个模块还支持对集合数据做聚合计算，比如直接求最大值、最小值等。

此外，我还使用过 **RedisGraph 扩展模块**。这个模块支持把图结构的数据保存到 Redis 中，并充分利用了 Redis 使用内存读写数据的性能优势，提供对图数据进行快速创建、查询和条件匹配。你要是感兴趣，可以看下 RedisGraph 的官网。

## 第 32 讲

**问题：**Redis 源码中还有一个针对 SDS 的小型测试框架，你知道这个测试框架是在哪个代码文件中吗?

这个小型测试框架是在 testhelp.h 文件中实现的。它定义了一个**宏 test_cond**，而这个宏实际是一段测试代码，它的参数包括了测试项描述 descr，以及具体的测试函数 _c。

这里，你需要注意的是，在这个小框架中，测试函数是作为 test_cond 参数传递的，这体现了函数式编程的思想，而且这种开发方式使用起来也很简洁。

下面的代码展示了这个小测试框架的主要部分，你可以看下。

```c
int __failed_tests = 0;  //失败的测试项的数目

int __test_num = 0;    //已测试项的数目

#define test_cond(descr,_c) do { \

    __test_num++; printf("%d - %s: ", __test_num, descr); \

    if(_c) printf("PASSED\n"); else {printf("FAILED\n"); __failed_tests++;} \  //运行测试函数_c，如果能通过，则打印PASSED，否则打印FAILED

} while(0);
```

那么，基于这个测试框架，在 sds.c 文件的 sdsTest 函数中，我就调用了 test_cond 宏，对 SDS 相关的多种操作进行了测试，你可以看看下面的示例代码。

```c
int sdsTest(void) {

    {

        sds x = sdsnew("foo");  //调用sdsnew创建一个sds变量x

        test_cond("Create a string and obtain the length",

  sdslen(x) == 3 && memcmp(x,"foo\0",4) == 0)  //调用test_cond测试sdsnew是否成功执行

   

        …

        x = sdscat(x,"bar");  //调用sdscat向sds变量x追求字符串

        test_cond("Strings concatenation",

  sdslen(x) == 5 && memcmp(x,"fobar\0",6) == 0); //调用test_cond测试sdscat是否成功执行

         …}
```

